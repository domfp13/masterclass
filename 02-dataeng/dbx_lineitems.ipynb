{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70f7c53b-1ffe-4d3e-a422-188b8c56306a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW EXTERNAL LOCATIONS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dae05a2-9e52-4852-aa67-221477415084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST 'abfss://warehouse@dbxdl.dfs.core.windows.net/lineitems/' WITH (CREDENTIAL `dbxdl-storage-account-creds`) LIMIT 3;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de184897-5c15-4052-b57f-89fdf7d14c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS MASTERCLASS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d767bd45-bb17-4f0c-b981-fb494b6b20c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG MASTERCLASS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f58383-d81b-41a1-8e6a-f4214deb5178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS BRONZE\n",
    " MANAGED LOCATION \"abfss://delta@dbxdl.dfs.core.windows.net/bronze/\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fc4520e-6f5c-4ed5-9fe5-3a669a02af4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE BRONZE.USEDCARS (\n",
    "  V VARIANT\n",
    ") USING DELTA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6069af-7a7b-4f4a-95d7-9d2bcc7d246f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM PARQUET.`abfss://warehouse@dbxdl.dfs.core.windows.net/lineitems/*`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459fa216-b5b5-4862-bf9a-055e3921b60f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "COPY INTO BRONZE.USEDCARS\n",
    "FROM 'abfss://warehouse@dbxdl.dfs.core.windows.net/lineitems/*'\n",
    "FILEFORMAT = PARQUET\n",
    "FORMAT_OPTIONS ('singleVariantColumn' = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a94114db-257b-42e6-b681-ef5c5dea8523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Why COPY INTO / Auto Loader don’t help here?\n",
    "* singleVariantColumn and the Auto Loader flag of the same name only exist for JSON sources. Therefor, a work around is needed. ￼ ￼\n",
    "* For Parquet you must therefore read the data with Spark (batch or structured streaming) and do the cast yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cc99cbc-1804-4c8b-a6e4-44734b5ced44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below is a workflow on  Databricks Runtime 15.3 (or later) / Serverless to load parquet into VARIANT, the first release that exposes the VARIANT type in Delta Lake. The key idea is that a Parquet record is first read as a STRUCT and then cast into a single VARIANT column; there is no “singleVariantColumn” option for Parquet the way there is for JSON, so you have to do the cast yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8155f441-df7c-4b08-a458-c6111e8d2117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1  Prerequisites\n",
    "\n",
    "| Requirement                                      | Why it matters                                      |\n",
    "|--------------------------------------------------|-----------------------------------------------------|\n",
    "| DBR ≥ 15.3                                       | VARIANT is only supported from 15.3 up              |\n",
    "| Delta table backed by Unity-Catalog or the hive metastore | VARIANT is a first-class Delta data type            |\n",
    "| Parquet rows ≤ 16 MB                             | Larger rows are silently redirected to _malformed_data |\n",
    "| Maps in your data must have string keys          | VARIANT rejects maps with non-string keys           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "014a8ed5-7ea5-474c-a218-fa5efc1979d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "TRUNCATE TABLE BRONZE.USEDCARS;\n",
    "\n",
    "INSERT INTO BRONZE.USEDCARS (V)\n",
    "SELECT parse_json(to_json(struct(*))) AS V\n",
    "FROM parquet.`abfss://warehouse@dbxdl.dfs.core.windows.net/lineitems/*`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14a65a2a-3b6e-4dc3-b236-085c2ed3cc4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM BRONZE.USEDCARS LIMIT 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1266e918-ab94-434b-8d61-ae938357c654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's try pulling out the attributes from the VARIANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e593aaa-1e68-487e-ab22-80a3f37db7f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT\n",
    "  variant_get(V, '$.L_ORDERKEY') AS order_key,\n",
    "  variant_get(V, '$.L_PARTKEY') AS part_key,\n",
    "  variant_get(V, '$.L_SUPPKEY') AS supp_key,\n",
    "  variant_get(V, '$.L_LINENUMBER') AS line_number,\n",
    "  variant_get(V, '$.L_QUANTITY') AS quantity,\n",
    "  variant_get(V, '$.L_EXTENDEDPRICE') AS extended_price,\n",
    "  variant_get(V, '$.L_DISCOUNT') AS discount,\n",
    "  variant_get(V, '$.L_TAX') AS tax,\n",
    "  variant_get(V, '$.L_RETURNFLAG') AS return_flag,\n",
    "  variant_get(V, '$.L_LINESTATUS') AS line_status,\n",
    "  variant_get(V, '$.L_SHIPDATE') AS ship_date,\n",
    "  variant_get(V, '$.L_COMMITDATE') AS commit_date,\n",
    "  variant_get(V, '$.L_RECEIPTDATE') AS receipt_date,\n",
    "  variant_get(V, '$.L_SHIPINSTRUCT') AS ship_instruct,\n",
    "  variant_get(V, '$.L_SHIPMODE') AS ship_mode,\n",
    "  variant_get(V, '$.L_COMMENT') AS comment\n",
    "FROM BRONZE.USEDCARS\n",
    "LIMIT 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dda25bdc-ed6d-4f8c-8732-dcbc060d5d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# What's the problem here?\n",
    "\n",
    "INSERT INTO … SELECT … FROM <files> is a generic SQL append: every time it runs it re‑reads whatever files you point it at and blindly appends those rows, so it has no memory of what it loaded before. COPY INTO, in contrast, is an idempotent, file‑aware loader; it maintains a load history (file path, size, checksum, timestamp) inside the Delta transaction log and therefore skips any file it has processed before, even if you rerun the same command or schedule it repeatedly. Databricks and Snowflake implement this the same way, which is why Snowflake’s COPY INTO also avoids duplicates while a vanilla INSERT INTO does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6e163ce-a988-4cba-a919-97ffea331579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# No DataFrame‑level substitute (and why)\n",
    "\n",
    "* There is no DataFrameWriter.copyInto() or similar; the only Spark‑side APIs are write.mode(...).save(...), which always append and therefore duplicate rows on re‑runs.  ￼\n",
    "* You could hand‑roll idempotency by writing input_file_name() into a “manifest” column and merging on it, but that just recreates what COPY INTO already does for you inside the Delta log. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "unity-catalog-acces-delta",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
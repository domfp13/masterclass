{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70f7c53b-1ffe-4d3e-a422-188b8c56306a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW EXTERNAL LOCATIONS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dae05a2-9e52-4852-aa67-221477415084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "LIST 'abfss://warehouse@dbxdl.dfs.core.windows.net/lineitems/' WITH (CREDENTIAL `dbxdl-storage-account-creds`) LIMIT 3;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de184897-5c15-4052-b57f-89fdf7d14c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS MASTERCLASS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d767bd45-bb17-4f0c-b981-fb494b6b20c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG MASTERCLASS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f58383-d81b-41a1-8e6a-f4214deb5178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS BRONZE\n",
    " MANAGED LOCATION \"abfss://delta@dbxdl.dfs.core.windows.net/bronze/\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fc4520e-6f5c-4ed5-9fe5-3a669a02af4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE BRONZE.RAW_LINEITEMS (\n",
    "  V VARIANT\n",
    ") USING DELTA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e6069af-7a7b-4f4a-95d7-9d2bcc7d246f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM PARQUET.`abfss://warehouse@dbxdl.dfs.core.windows.net/lineitems/*` LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459fa216-b5b5-4862-bf9a-055e3921b60f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "COPY INTO BRONZE.RAW_LINEITEMS\n",
    "FROM (\n",
    "  SELECT parse_json(to_json(struct(*))) AS V \n",
    "  FROM 'abfss://warehouse@dbxdl.dfs.core.windows.net/lineitems/*'\n",
    ")\n",
    "FILEFORMAT = PARQUET\n",
    "FORMAT_OPTIONS ('singleVariantColumn' = 'true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7886e9-52bf-4cc5-b5d8-6d7d8109cf50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT COUNT(*) FROM BRONZE.RAW_LINEITEMS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1410a161-a48b-4466-8b8e-f75eeaa3c2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  variant_get(V, '$.L_ORDERKEY') AS order_key,\n",
    "  variant_get(V, '$.L_PARTKEY') AS part_key,\n",
    "  variant_get(V, '$.L_SUPPKEY') AS supp_key,\n",
    "  variant_get(V, '$.L_LINENUMBER') AS line_number,\n",
    "  variant_get(V, '$.L_QUANTITY') AS quantity,\n",
    "  variant_get(V, '$.L_EXTENDEDPRICE') AS extended_price,\n",
    "  variant_get(V, '$.L_DISCOUNT') AS discount,\n",
    "  variant_get(V, '$.L_TAX') AS tax,\n",
    "  variant_get(V, '$.L_RETURNFLAG') AS return_flag,\n",
    "  variant_get(V, '$.L_LINESTATUS') AS line_status,\n",
    "  variant_get(V, '$.L_SHIPDATE') AS ship_date,\n",
    "  variant_get(V, '$.L_COMMITDATE') AS commit_date,\n",
    "  variant_get(V, '$.L_RECEIPTDATE') AS receipt_date,\n",
    "  variant_get(V, '$.L_SHIPINSTRUCT') AS ship_instruct,\n",
    "  variant_get(V, '$.L_SHIPMODE') AS ship_mode,\n",
    "  variant_get(V, '$.L_COMMENT') AS comment\n",
    "FROM BRONZE.RAW_LINEITEMS\n",
    "LIMIT 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6e163ce-a988-4cba-a919-97ffea331579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# No DataFrame‑level substitute (and why)\n",
    "\n",
    "* There is no DataFrameWriter.copyInto() or similar; the only Spark‑side APIs are write.mode(...).save(...), which always append and therefore duplicate rows on re‑runs.  ￼\n",
    "* You could hand‑roll idempotency by writing input_file_name() into a “manifest” column and merging on it, but that just recreates what COPY INTO already does for you inside the Delta log. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b0c3ebe-40ca-4d78-8b2b-b40d04ad222a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SQL Serverless Photon Engine only SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7de7aeee-1cee-4242-a51a-fdbe74d0ed44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG MASTERCLASS;\n",
    "USE SCHEMA BRONZE;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS LINEITEMS (\n",
    "  PK_ID                BIGINT,\n",
    "  L_ORDERKEY           STRING,\n",
    "  L_PARTKEY            STRING,\n",
    "  L_SUPPKEY            STRING,\n",
    "  L_LINENUMBER         STRING,\n",
    "  L_QUANTITY           STRING,\n",
    "  L_EXTENDEDPRICE      STRING,\n",
    "  L_DISCOUNT           STRING,\n",
    "  L_TAX                STRING,\n",
    "  L_RETURNFLAG         STRING,\n",
    "  L_LINESTATUS         STRING,\n",
    "  L_SHIPDATE           STRING,\n",
    "  L_COMMITDATE         STRING,\n",
    "  L_RECEIPTDATE        STRING,\n",
    "  L_SHIPINSTRUCT       STRING,\n",
    "  L_SHIPMODE           STRING,\n",
    "  L_COMMENT            STRING,\n",
    "  INSERTED_DATE        TIMESTAMP\n",
    ")\n",
    "USING DELTA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbaa0fa8-2ccd-434c-a43d-4a1c4cfa5457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, current_timestamp, monotonically_increasing_id, col\n",
    "\n",
    "# 1️⃣ Read all your Parquet files from the mounted stage\n",
    "df = (\n",
    "    spark.read.parquet(\"abfss://warehouse@dbxdl.dfs.core.windows.net/lineitems/*\")\n",
    ")\n",
    "\n",
    "stringified = df.select(\n",
    "    *[col(c).cast(\"string\").alias(c) for c in df.columns],\n",
    "    monotonically_increasing_id().alias(\"PK_ID\"),\n",
    "    current_timestamp().alias(\"INSERTED_DATE\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d59d057-769a-47a1-90be-be881a7f855e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(stringified.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba81fdb-8720-4fd1-ae13-17289476a190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(stringified\n",
    "   .write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"append\")\n",
    "   .saveAsTable(\"LINEITEMS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bd9f8b7-4bcb-43cc-bce7-08536f275530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delta’s Python API doesn’t let you call .mode(\"merge\") on the DataFrameWriter—but it's possible to do a Delta MERGE (upsert).  DeltaTable .merge() is about upserting rows, not about “only ingesting new files” the way Snowflake’s COPY INTO … FORCE=FALSE works.  The closest equivalent in Databricks is Autoloader, because it keeps its own file‐tracking checkpoint and by default only processes each file once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b888540-4f4b-4cb1-80a9-807ebfdebe77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0e6381a-d1d1-4adf-be14-898851d6db1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, input_file_name, input_file_modification_time, current_timestamp, monotonically_increasing_id\n",
    "\n",
    "# 1️⃣ Read & prepare your incoming data\n",
    "df = (\n",
    "    spark.read.parquet(\"abfss://warehouse@dbxdl.dfs.core.windows.net/lineitems/*\")\n",
    ")\n",
    "\n",
    "stringified = df.select(\n",
    "    *[col(c).cast(\"string\").alias(c) for c in df.columns],\n",
    "    monotonically_increasing_id().alias(\"PK_ID\"),\n",
    "    current_timestamp().alias(\"INSERTED_DATE\")\n",
    ")\n",
    "\n",
    "# 2️⃣ Reference the target Delta table\n",
    "delta_table = DeltaTable.forName(spark, \"LINEITEMS\")\n",
    "\n",
    "# 3️⃣ Merge: match on whatever business keys make sense,\n",
    "#    here we’re using FILE_NAME + PK_ID as an example\n",
    "# (delta_table.alias(\"t\")\n",
    "#   .merge(\n",
    "#      source = stringified.alias(\"s\"),\n",
    "#      condition = \"t.FILE_NAME = s.FILE_NAME AND t.PK_ID = s.PK_ID\"\n",
    "#   )\n",
    "#   .whenMatchedUpdateAll()      # update every column in the target from source\n",
    "#   .whenNotMatchedInsertAll()   # insert any new rows\n",
    "#   .execute()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6d8b5a4-84e9-457a-bca5-d016c9e5fedb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Autoloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a3d6773-a81b-4a2f-9de4-f3443e271e50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "With Autoloader and then kick off a streaming write (either via .start() or as part of a Databricks Job), you’re launching a long-running Structured Streaming query inside your Spark cluster. It isn’t “sent off” to some external queue—Spark itself tracks file progress and runs the work in the background of that cluster until you stop it.\n",
    "\n",
    "you’ll get back a StreamingQuery object. The driver will continue scheduling micro-batches in the background, and you can even go do other cells or check query.status. But if you detach or idle-timeout that notebook session, the cluster will eventually shut down and your stream stops.\n",
    "\n",
    "\t•\tAs a Job: the recommended production pattern is to wrap your Autoloader code in a notebook (or .py file) and then create a Databricks Job that runs it. A job cluster stays alive for the duration of the streaming query (up to whatever timeout you configure), so your Autoloader will keep ingesting new files without you having to keep a notebook tab open.\n",
    "\n",
    "\t2.\tCluster uptime\n",
    "\t•\tStreaming queries only run while the cluster is up. If you’re using a job cluster, it spins up at job start and only terminates if the job fails or completes.\n",
    "\t•\tIf you tried to run it on your interactive workspace cluster and leave the notebook running, auto-termination or idle timeouts will eventually kill it.\n",
    "\t3.\tMonitoring & failure recovery\n",
    "\t•\tBecause Autoloader checkpoints file progress to the checkpointLocation, if the stream dies (cluster restart, transient error), when you restart the same query pointing at the same checkpoint it will resume exactly where it left off—never reprocessing old files.\n",
    "\t•\tYou can monitor the job from the Databricks Jobs UI (micro-batch durations, backpressure alerts, etc.).\n",
    "\n",
    "⸻\n",
    "\n",
    "So, do you need to keep a notebook open?\n",
    "\t•\tNo—you should deploy it as a Databricks Job (or Delta Live Table pipeline) for reliable background execution.\n",
    "\t•\tIf you’re just experimenting, you can run it in an interactive notebook, but be aware the stream stops as soon as that cluster shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04bd2dae-13a5-41ef-af25-878ff8de0904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import input_file_name, input_file_modification_time, current_timestamp, monotonically_increasing_id\n",
    "\n",
    "# 1️⃣ Define the streaming source with Autoloader\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "         .format(\"cloudFiles\")\n",
    "         .option(\"cloudFiles.format\", \"parquet\")\n",
    "         .option(\"cloudFiles.schemaLocation\", \"/mnt/checkpoints/lineitems/schema\")      # for schema inference\n",
    "         .load(\"dbfs:/mnt/data_stage\")                                                # your mounted path\n",
    ")\n",
    "\n",
    "# 2️⃣ Enrich with your synthetic PK + metadata\n",
    "enriched = (\n",
    "    streaming_df\n",
    "      .withColumn(\"PK_ID\",               monotonically_increasing_id())\n",
    "      .withColumn(\"FILE_NAME\",           input_file_name())\n",
    "      .withColumn(\"FILE_LAST_MODIFIED\",  input_file_modification_time())\n",
    "      .withColumn(\"INSERTED_DATE\",       current_timestamp())\n",
    ")\n",
    "\n",
    "# 3️⃣ In each micro-batch, MERGE the new rows into your target Delta table\n",
    "def upsert_batch(batch_df, batch_id):\n",
    "    delta_tbl = DeltaTable.forName(spark, \"LINEITEMS\")\n",
    "    (delta_tbl.alias(\"t\")\n",
    "         .merge(\n",
    "            source    = batch_df.alias(\"s\"),\n",
    "            condition = \"t.FILE_NAME = s.FILE_NAME AND t.PK_ID = s.PK_ID\"\n",
    "         )\n",
    "         .whenMatchedUpdateAll()\n",
    "         .whenNotMatchedInsertAll()\n",
    "         .execute()\n",
    "    )\n",
    "\n",
    "# 4️⃣ Wire it all together in a streaming query\n",
    "(\n",
    "  enriched.writeStream\n",
    "          .format(\"delta\")\n",
    "          .foreachBatch(upsert_batch)\n",
    "          .option(\"checkpointLocation\", \"/mnt/checkpoints/lineitems/merge\")  \n",
    "          .outputMode(\"update\")           # or \"append\"—checkpoint drives exactly-once\n",
    "          .start()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7296832481473026,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "dbx_lineitems",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
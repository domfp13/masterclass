{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3637b43b-c070-4170-8649-3d418b69ba7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "jdbc_hostname = dbutils.secrets.get(scope='dbx-scope', key='oracle-hostname')      # e.g. \"db.myvps.example.com\"\n",
    "jdbc_port     = \"1521\"\n",
    "service_name  = dbutils.secrets.get(scope='dbx-scope', key='oracle-service-name')  # e.g. \"SAMPLEFREEPDB1\"\n",
    "jdbc_user     = dbutils.secrets.get(scope='dbx-scope', key='oracle-user')\n",
    "jdbc_password = dbutils.secrets.get(scope='dbx-scope', key='oracle-password')\n",
    "\n",
    "jdbc_url = f\"jdbc:oracle:thin:@//{jdbc_hostname}:{jdbc_port}/{service_name}\"\n",
    "driver   = \"oracle.jdbc.OracleDriver\"   # from ojdbc17.jar installed in the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35cf5e3a-2d38-4545-8dc7-1c16e5b19f40",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757241348512}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df = (spark.read.format(\"jdbc\")\n",
    "  .option(\"url\", jdbc_url)\n",
    "  .option(\"dbtable\", \"(SELECT * FROM ORDERS WHERE ROWNUM <= 10) t\")\n",
    "  .option(\"user\", jdbc_user)\n",
    "  .option(\"password\", jdbc_password)\n",
    "  .option(\"driver\", driver)\n",
    "  .option(\"fetchsize\", \"10\")\n",
    "  .load())\n",
    "\n",
    "display(test_df)   # or: test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77c1bfe4-41bf-4a3c-9106-1cca07069c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CDC from Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b288173-a6a6-4e72-ab05-03e6d21150aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "jdbc_url    = f\"jdbc:oracle:thin:@//{jdbc_hostname}:{jdbc_port}/{service_name}\"\n",
    "jdbc_driver = \"oracle.jdbc.OracleDriver\"\n",
    "\n",
    "source_table = \"ORDERS\"            # add schema if needed, e.g. MYSCHEMA.ORDERS\n",
    "pk_col       = \"ORDER_ID\"\n",
    "ts_col       = \"ORDER_TIMESTAMP\"\n",
    "\n",
    "target_table    = \"delta_db.orders_delta\"\n",
    "watermark_table = \"delta_db._wm_orders\"\n",
    "\n",
    "batch_limit_rows       = 10000\n",
    "safety_lag_seconds     = 30\n",
    "max_retries            = 5\n",
    "\n",
    "jdbc_read_options = {\"fetchsize\": \"100\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "832df3d3-e77e-4539-8ab2-680d26258127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE DATABASE DELTA_DB;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7bdaf28-4c0c-4146-a124-45da171b72f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the target with the real schema\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS delta_db.orders_delta (\n",
    "  ORDER_ID        BIGINT,\n",
    "  ORDER_UUID      STRING,\n",
    "  TEMPERATURE     DOUBLE,\n",
    "  ORDER_TIMESTAMP TIMESTAMP\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Create WM table if missing\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS delta_db._wm_orders (\n",
    "  table_name STRING,\n",
    "  last_ts    TIMESTAMP,\n",
    "  last_id    BIGINT\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Initialize watermark if empty\n",
    "if spark.table(\"delta_db._wm_orders\").where(\"table_name = 'ORDERS'\").count() == 0:\n",
    "    spark.sql(\"\"\"\n",
    "      INSERT INTO delta_db._wm_orders VALUES ('ORDERS', TIMESTAMP '1970-01-01 00:00:00', 0)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f559c1-dd3d-43a3-ae84-f189783559ff",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757244272070}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "display(spark.table('_wm_orders').limit(20))\n",
    "display(spark.table('orders_delta').limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f84590af-1e1e-4948-8770-a5f088a493f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def get_watermark():\n",
    "    row = (spark.table(watermark_table)\n",
    "           .where(\"table_name = 'ORDERS'\")\n",
    "           .select(\"last_ts\", \"last_id\")\n",
    "           .first())\n",
    "    return row[\"last_ts\"], int(row[\"last_id\"])\n",
    "\n",
    "def set_watermark(new_ts, new_id):\n",
    "    spark.sql(f\"\"\"\n",
    "      MERGE INTO {watermark_table} AS t\n",
    "      USING (SELECT 'ORDERS' AS table_name,\n",
    "                    TIMESTAMP '{new_ts.strftime(\"%Y-%m-%d %H:%M:%S\")}' AS last_ts,\n",
    "                    {new_id} AS last_id) AS s\n",
    "      ON t.table_name = s.table_name\n",
    "      WHEN MATCHED THEN UPDATE SET last_ts = s.last_ts, last_id = s.last_id\n",
    "      WHEN NOT MATCHED THEN INSERT (table_name, last_ts, last_id)\n",
    "           VALUES (s.table_name, s.last_ts, s.last_id)\n",
    "    \"\"\")\n",
    "\n",
    "def read_incremental_batch(last_ts, last_id, limit_rows, safety_lag_s):\n",
    "    upper_ts   = (datetime.now(timezone.utc) - timedelta(seconds=safety_lag_s)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    last_ts_str= last_ts.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Cast ORDER_ID to integer width so it matches BIGINT in the target/watermark\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "      CAST({pk_col} AS NUMBER(38,0)) AS {pk_col},\n",
    "      ORDER_UUID,\n",
    "      TEMPERATURE,\n",
    "      {ts_col}\n",
    "    FROM {source_table}\n",
    "    WHERE\n",
    "      ( {ts_col} > TO_TIMESTAMP('{last_ts_str}', 'YYYY-MM-DD HH24:MI:SS')\n",
    "        OR ( {ts_col} = TO_TIMESTAMP('{last_ts_str}', 'YYYY-MM-DD HH24:MI:SS')\n",
    "             AND CAST({pk_col} AS NUMBER(38,0)) > {last_id} )\n",
    "      )\n",
    "      AND {ts_col} <= TO_TIMESTAMP('{upper_ts}', 'YYYY-MM-DD HH24:MI:SS')\n",
    "    ORDER BY {ts_col}, CAST({pk_col} AS NUMBER(38,0))\n",
    "    FETCH FIRST {limit_rows} ROWS ONLY\n",
    "    \"\"\"\n",
    "\n",
    "    reader = (spark.read.format(\"jdbc\")\n",
    "              .option(\"url\", jdbc_url)\n",
    "              .option(\"dbtable\", f\"({query}) t\")\n",
    "              .option(\"user\", jdbc_user)\n",
    "              .option(\"password\", jdbc_password)\n",
    "              .option(\"driver\", jdbc_driver))\n",
    "    for k,v in jdbc_read_options.items():\n",
    "        reader = reader.option(k,v)\n",
    "    return reader.load()\n",
    "\n",
    "def cdc_once():\n",
    "    last_ts, last_id = get_watermark()\n",
    "    df = read_incremental_batch(last_ts, last_id, batch_limit_rows, safety_lag_seconds)\n",
    "\n",
    "    if df.head(1) == []:\n",
    "        return 0, last_ts, last_id\n",
    "\n",
    "    # Idempotent upsert. Columns on both sides match the target we created above.\n",
    "    df.createOrReplaceTempView(\"stg_orders\")\n",
    "    spark.sql(f\"\"\"\n",
    "      MERGE INTO {target_table} AS tgt\n",
    "      USING stg_orders AS src\n",
    "      ON tgt.{pk_col} = src.{pk_col}\n",
    "      WHEN MATCHED THEN UPDATE SET\n",
    "        tgt.ORDER_ID        = src.ORDER_ID,\n",
    "        tgt.ORDER_UUID      = src.ORDER_UUID,\n",
    "        tgt.TEMPERATURE     = src.TEMPERATURE,\n",
    "        tgt.ORDER_TIMESTAMP = src.ORDER_TIMESTAMP\n",
    "      WHEN NOT MATCHED THEN INSERT (\n",
    "        ORDER_ID, ORDER_UUID, TEMPERATURE, ORDER_TIMESTAMP\n",
    "      ) VALUES (\n",
    "        src.ORDER_ID, src.ORDER_UUID, src.TEMPERATURE, src.ORDER_TIMESTAMP\n",
    "      )\n",
    "    \"\"\")\n",
    "\n",
    "    max_ts = df.agg(F.max(F.col(ts_col)).alias(\"m\")).collect()[0][\"m\"]\n",
    "    max_id = (df.filter(F.col(ts_col) == F.lit(max_ts))\n",
    "                .agg(F.max(F.col(pk_col)).alias(\"id\")).collect()[0][\"id\"])\n",
    "\n",
    "    set_watermark(max_ts, int(max_id))\n",
    "    return df.count(), max_ts, int(max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e347feca-5939-49ed-8efb-5e5c0cc1b72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64597b26-36ba-4c39-b415-2d8d85de56b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rows, new_ts, new_id = cdc_once()\n",
    "print(f\"[TEST] Upserted {rows} rows; watermark → {new_ts} / {new_id}\")\n",
    "\n",
    "display(spark.table(target_table).limit(20))\n",
    "display(spark.table(watermark_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5e2fadd-bbbc-4cca-a57f-c6d6cee6dc5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Oracle → Databricks CDC Approaches\n",
    "\n",
    "## Option 1: Polling as a Job (Scheduled|Continuous)\n",
    "\n",
    "In this approach, the CDC process is executed once per run and scheduled using Databricks Jobs.  \n",
    "The job runs on a defined interval (for example, every minute), queries Oracle for new changes since the last watermark, and updates the Delta table.  \n",
    "Each execution starts, processes the incremental data, updates the watermark, and then stops.\n",
    "\n",
    "**Characteristics:**\n",
    "- Latency depends on the schedule (e.g., 1–5 minutes).  \n",
    "- Cost-efficient: clusters can terminate between runs.  \n",
    "- Easy to monitor and retry using the Databricks Jobs interface.  \n",
    "- Good for demos and PoCs where near-real-time (not sub-second) is acceptable.  \n",
    "\n",
    "---\n",
    "\n",
    "## Option 2: Infinite Loop (Always-On)\n",
    "\n",
    "In this approach, the CDC process runs continuously inside a notebook or job.  \n",
    "A `while` loop keeps polling Oracle on a short interval, applying changes to the Delta table and updating the watermark without stopping.  \n",
    "The notebook remains active and constantly looks for new changes.\n",
    "\n",
    "**Characteristics:**\n",
    "- Lower latency, as changes are picked up almost immediately.  \n",
    "- Requires a long-running cluster, which increases cost.  \n",
    "- Needs timeout/restart configuration to ensure resiliency.  \n",
    "- Useful for demos where continuous updates must be visible without waiting for the next job run.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37420b54-2589-455e-b743-078f4e95642a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hard reset\n",
    "#spark.sql(\"DROP TABLE IF EXISTS delta_db.orders_delta\")\n",
    "#spark.sql(\"DROP TABLE IF EXISTS delta_db._wm_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d34b6b93-1117-4782-9353-ff2fbda9b1bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "oracle-cdc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}